---
title: "Covid Forecast Model Selection"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(EpiSoon)
evaluations <- readRDS("forecast_evaluation_21.rds")

```

## Workplan

## Overview Scoring Rules {.tabset}

### Bias
should be around 0.5 (between -1 and 1 according to Gunnar, but that seems just wrong)

### Sharpness
Median Absolute Deviation, i.e., the (lo-/hi-) median of the absolute deviations from the median. 
Smaller is better. 

### CRPS
generalisation of Brier score to continuous variables. Smaller is better. 

### LogS

**Advantage**: logarithmic Score penalises underestimating uncertainty heavily. I feel this is what we want. 

**Drawback**: In contrast to the CRPS, the computation of LogS requires a predictive density. An estimatorcan be obtained with classical nonparametric kernel density estimation (KDE, e.g. Silverman1986). However, this estimator is valid only under stringent theoretical assumptions and canbe fragile in practice: If the outcome falls into the tails of the simulated forecast distribution,the estimated score may be highly sensitive to the choice of the bandwidth tuning parameter.In an MCMC context, a mixture-of-parameters estimator that utilizes a simulated sampleof parameter draws rather than draws from the posterior predictive distribution is a better

--> especially problematic I think if we work with traces and only small sample sizes?

**Question** do we now the posterior distribution of our draws?

### DSS
proper scoring rule that only depends on the first two moments. Smaller is better. 



## 
![Correlation between different metrics](results/plots/correlation_metrics_different_horizons.png)

## Predictions over time horizons {.tabset}
### 1 day ahead 
![1 day ahead prediction](results/plots/forecasts_1_ahead_all_regions.png)

### 3 days ahead 
![3 day ahead prediction](results/plots/forecasts_3_ahead_all_regions.png)

### 7 days ahead 
![7 day ahead prediction](results/plots/forecasts_7_ahead_all_regions.png)

### 14 days ahead 
![14 day ahead prediction](results/plots/forecasts_14_ahead_all_regions.png)

### 21 days ahead 
![21 day ahead prediction](results/plots/forecasts_21_ahead_all_regions.png)

## Scoring by metrics {.tabset}

### Bias
![median Bias across time](results/plots/bias_median_across_regions_all_horizons.png)

![Bias across different time horizons](results/plots/biasacross_regions_all_horizons.png)

![Bias across different time horizons](results/plots/biasBias_violin_across_regions_different_horizons.png)


###  CRPS
![median CRPS across time](results/plots/crps_median_across_regions_all_horizons.png)

![CRPS across different time horizons](results/plots/crpsacross_regions_all_horizons.png)

### logS
![median LogS across time](results/plots/logs_median_across_regions_all_horizons.png)

![LogS across different time horizons](results/plots/logsacross_regions_all_horizons.png)

### DSS
![median DSS across time](results/plots/dss_median_across_regions_all_horizons.png)

![DSS across different time horizons](results/plots/dssacross_regions_all_horizons.png)

### Sharpness
![median Sharpness across time](results/plots/sharpness_median_across_regions_all_horizons.png)

![Sharpness across different time horizons](results/plots/sharpnessacross_regions_all_horizons.png)


## Scoring by days {.tabset}
```{r horizons, include=FALSE}
score_by_days <- summarise_scores(evaluations$scores, variables = "horizon")

```

### 1 day ahead

##### **Conclusion**
Across the bench, Sparse AR seems the most reasonable (little bias, ok DSS and LogS)

AR1 seems to be very unconfident (and therefore performs well on LogS)
AR1 seems to be downward biased. 



![Bias across models 1 day ahead](results/plots/bias_across_regions_1_day_ahead_forecasts.png)

![CRPS across models 1 day ahead](results/plots/crps_across_regions_1_day_ahead_forecasts.png)

![LogS across models 1 day ahead](results/plots/logs_across_regions_1_day_ahead_forecasts.png)

![DSS across models 1 day ahead](results/plots/dss_across_regions_1_day_ahead_forecasts.png)

![Sharpness across models 1 day ahead](results/plots/sharpness_across_regions_1_day_ahead_forecasts.png)



```{r horizon-1, echo=FALSE}
score_by_days %>% dplyr::filter(horizon == 1) %>% 
  knitr::kable(booktabs = TRUE, linesep = c("", "", "", "\\hline"))

```


### 3 day ahead 

#### Scoring

##### **Conclusion**
AR1 seems very bad in terms of bias and everything. 

Sparse AR is the best in terms of crps, AR1 the worst. 
Sparse AR also best with dss

--> take Sparse AR

All models have a tendency to be downwards biased, the local and semilocal ones tend to do a bit better. 

``` {r, echo = F, eval = F}
horizon <- 3
metrics <- c("bias", "crps", "logs", "dss", "sharpness")
for (metric in metrics) {
  
  file <- paste("results/plots/", metric, "bias", "_across_regions_", horizon, 
                 "_day_ahead_forecasts.png", sep = "")

  cat("![Bias across model forecasts](", file, ")\n \n", sep = "")
}

t <- horizon
score_by_days %>% dplyr::filter(horizon == t) %>% 
  knitr::kable(booktabs = TRUE, linesep = c("", "", "", "\\hline"))

```

![Bias across models 3 day ahead](results/plots/bias_across_regions_3_day_ahead_forecasts.png)

![CRPS across models 3 day ahead](results/plots/crps_across_regions_3_day_ahead_forecasts.png)

![LogS across models 3 day ahead](results/plots/logs_across_regions_3_day_ahead_forecasts.png)

![DSS across models 3 day ahead](results/plots/dss_across_regions_3_day_ahead_forecasts.png)

![Sharpness across models 3 day ahead](results/plots/sharpness_across_regions_3_day_ahead_forecasts.png)



```{r horizon-3, echo=FALSE}
score_by_days %>% dplyr::filter(horizon == 3) %>% 
  knitr::kable(booktabs = TRUE, linesep = c("", "", "", "\\hline"))
```


### 7 days ahead 

``` {r, echo = F, results = "asis"}
horizon <- 7
metrics <- c("bias", "crps", "logs", "dss", "sharpness")
for (metric in metrics) {
  
  file <- paste("results/plots/", metric, "_across_regions_", horizon, 
                 "_day_ahead_forecasts.png", sep = "")

  cat("![Bias across model forecasts](", file, ")\n \n", sep = "")
}

t <- horizon
score_by_days %>% dplyr::filter(horizon == t) %>% 
  knitr::kable(booktabs = TRUE, linesep = c("", "", "", "\\hline"))

```

### 14 days ahead 

``` {r, echo = F, results = "asis"}
horizon <- 14
metrics <- c("bias", "crps", "logs", "dss", "sharpness")
for (metric in metrics) {
  
  file <- paste("results/plots/", metric, "_across_regions_", horizon, 
                 "_day_ahead_forecasts.png", sep = "")

  cat("![Bias across model forecasts](", file, ")\n \n", sep = "")
}

t <- horizon
score_by_days %>% dplyr::filter(horizon == t) %>% 
  knitr::kable(booktabs = TRUE, linesep = c("", "", "", "\\hline"))

```


### 21 days ahead

``` {r, echo = F, results = "asis"}
horizon <- 21
metrics <- c("bias", "crps", "logs", "dss", "sharpness")
for (metric in metrics) {
  
  file <- paste("results/plots/", metric, "_across_regions_", horizon, 
                 "_day_ahead_forecasts.png", sep = "")

  cat("![Bias across model forecasts](", file, ")\n \n", sep = "")
}

t <- horizon
score_by_days %>% dplyr::filter(horizon == t) %>% 
  knitr::kable(booktabs = TRUE, linesep = c("", "", "", "\\hline"))

```



## Performance by countries {.tabset}
``` {r, echo = F, results = "asis"}
horizons <- c(1, 3, 7, 14, 21)
metrics <- c("crps", "logs", "dss")
for (t in horizons) {
  title <- paste("### ", t, "day(s) ahead")
  cat(title, "\n \n")
  for (metric in metrics) {
    file <- paste("results/plots/regions_", metric, "_", t, 
                 "_ahead.png", sep = "")
    cat("![](", file, ")\n \n", sep = "")
  }
}
```

## Region performance vs metrics {.tabset}

``` {r, echo = F, results = "asis"}

files <- list.files("results/plots/tests/")

titles <- files %>% 
  stringr::str_remove(pattern = c("pred_1_")) %>%
  stringr::str_remove(pattern = c("_vs_scores.png"))

for (i in 1:length(files)) {
  title <- paste("###", titles[i])
  cat(title, "\n \n")
  pic <- paste("![](results/plots/tests/", files[i], ")", sep = "")
  cat(pic, "\n \n")
}
                      
```

